import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import re
import traceback
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, Birch
from sklearn.mixture import GaussianMixture
from sklearn.metrics import (
    silhouette_score, calinski_harabasz_score, davies_bouldin_score,
    adjusted_rand_score, adjusted_mutual_info_score, homogeneity_score,
    completeness_score, v_measure_score, fowlkes_mallows_score
)
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.manifold import TSNE
from sklearn.neighbors import NearestNeighbors

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è NLP
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
from rank_bm25 import BM25Okapi
import hdbscan
import umap.umap_ as umap
from sentence_transformers import SentenceTransformer
import gensim
from gensim.models import Word2Vec, FastText

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö NLTK
try:
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
except:
    pass

class AdvancedTextProcessor:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    
    def __init__(self):
        try:
            self.stemmer = SnowballStemmer("russian")
            self.stop_words = set(stopwords.words('russian'))
        except:
            # –†–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
            self.stemmer = None
            self.stop_words = {
                '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', 
                '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', 
                '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', 
                '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', 
                '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', 
                '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', 
                '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', 
                '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', 
                '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', 
                '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', 
                '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', 
                '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', 
                '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', 
                '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', 
                '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', 
                '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', 
                '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'
            }
    
    def clean_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not isinstance(text, str):
            return ""
        text = re.sub(r'[^–∞-—è—ëa-z\s]', ' ', text.lower())
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def process_word(self, word, use_stemming=True):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞"""
        if len(word) > 2 and word not in self.stop_words:
            if use_stemming and self.stemmer:
                try:
                    return self.stemmer.stem(word)
                except:
                    return word
            return word
        return None
    
    def tokenize(self, text, use_stemming=True):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        text = self.clean_text(text)
        if not text:
            return []
        
        tokens = text.split()
        processed_tokens = []
        
        for token in tokens:
            processed_word = self.process_word(token, use_stemming)
            if processed_word:
                processed_tokens.append(processed_word)
        
        return processed_tokens
    
    def preprocess_texts(self, texts, use_stemming=True):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        processed_texts = []
        for text in texts:
            tokens = self.tokenize(text, use_stemming)
            processed_texts.append(tokens)
        return processed_texts

class AdvancedVectorizer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
    
    def __init__(self):
        self.vectorizer = None
        self.bm25 = None
        self.sentence_model = None
        self.word2vec_model = None
        self.fasttext_model = None
    
    def fit_tfidf(self, tokenized_texts, max_features=5000, ngram_range=(1, 2)):
        """TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"""
        texts = [' '.join(tokens) for tokens in tokenized_texts]
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            min_df=2,
            max_df=0.8,
            ngram_range=ngram_range
        )
        return self.vectorizer.fit_transform(texts).toarray()
    
    def fit_count(self, tokenized_texts, max_features=5000, ngram_range=(1, 2)):
        """Count –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"""
        texts = [' '.join(tokens) for tokens in tokenized_texts]
        self.vectorizer = CountVectorizer(
            max_features=max_features,
            min_df=2,
            max_df=0.8,
            ngram_range=ngram_range
        )
        return self.vectorizer.fit_transform(texts).toarray()
    
    def fit_bm25(self, tokenized_texts):
        """BM25 –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"""
        self.bm25 = BM25Okapi(tokenized_texts)
        vectors = []
        for doc in tokenized_texts:
            vectors.append(self.bm25.get_scores(doc))
        return np.array(vectors)
    
    def fit_sentence_transformers(self, texts, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):
        """–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Sentence Transformers"""
        try:
            self.sentence_model = SentenceTransformer(model_name)
            return self.sentence_model.encode(texts)
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ Sentence Transformers: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º TF-IDF –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
            return self.fit_tfidf([[' '.join(text.split()[:10])] for text in texts])
    
    def fit_word2vec(self, tokenized_texts, vector_size=100, window=5, min_count=2):
        """Word2Vec –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"""
        try:
            self.word2vec_model = Word2Vec(
                sentences=tokenized_texts,
                vector_size=vector_size,
                window=window,
                min_count=min_count,
                workers=4
            )
            
            # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤
            doc_vectors = []
            for tokens in tokenized_texts:
                vectors = []
                for token in tokens:
                    if token in self.word2vec_model.wv:
                        vectors.append(self.word2vec_model.wv[token])
                if vectors:
                    doc_vectors.append(np.mean(vectors, axis=0))
                else:
                    doc_vectors.append(np.zeros(vector_size))
            
            return np.array(doc_vectors)
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ Word2Vec: {e}")
            return self.fit_tfidf(tokenized_texts)
    
    def fit_fasttext(self, tokenized_texts, vector_size=100, window=5, min_count=2):
        """FastText –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"""
        try:
            self.fasttext_model = FastText(
                sentences=tokenized_texts,
                vector_size=vector_size,
                window=window,
                min_count=min_count,
                workers=4
            )
            
            # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤
            doc_vectors = []
            for tokens in tokenized_texts:
                vectors = []
                for token in tokens:
                    if token in self.fasttext_model.wv:
                        vectors.append(self.fasttext_model.wv[token])
                if vectors:
                    doc_vectors.append(np.mean(vectors, axis=0))
                else:
                    doc_vectors.append(np.zeros(vector_size))
            
            return np.array(doc_vectors)
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ FastText: {e}")
            return self.fit_tfidf(tokenized_texts)
    
    def fit_doc2vec(self, tokenized_texts, vector_size=100, window=5, min_count=2):
        """Doc2Vec-like –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Word2Vec –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º
        return self.fit_word2vec(tokenized_texts, vector_size, window, min_count)

class AdvancedClusteringMethods:
    """–ö–ª–∞—Å—Å —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    
    @staticmethod
    def kmeans(vectors, n_clusters=8, random_state=42):
        model = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
        labels = model.fit_predict(vectors)
        return labels, model
    
    @staticmethod
    def dbscan(vectors, eps=0.5, min_samples=5):
        vectors_norm = normalize(vectors)
        model = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
        labels = model.fit_predict(vectors_norm)
        return labels, model
    
    @staticmethod
    def hierarchical(vectors, n_clusters=8, linkage='ward'):
        model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)
        labels = model.fit_predict(vectors)
        return labels, model
    
    @staticmethod
    def gaussian_mixture(vectors, n_components=8, random_state=42):
        model = GaussianMixture(n_components=n_components, random_state=random_state)
        labels = model.fit_predict(vectors)
        return labels, model
    
    @staticmethod
    def spectral(vectors, n_clusters=8, random_state=42):
        model = SpectralClustering(n_clusters=n_clusters, random_state=random_state, 
                                 affinity='nearest_neighbors', n_neighbors=10)
        labels = model.fit_predict(vectors)
        return labels, model
    
    @staticmethod
    def hdbscan_method(vectors, min_cluster_size=5):
        model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean')
        labels = model.fit_predict(vectors)
        return labels, model
    
    @staticmethod
    def birch(vectors, n_clusters=8, threshold=0.5):
        model = Birch(n_clusters=n_clusters, threshold=threshold)
        labels = model.fit_predict(vectors)
        return labels, model

class MetricsCalculator:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    
    @staticmethod
    def calculate_all_metrics(vectors, labels, true_labels=None):
        """–†–∞—Å—á–µ—Ç –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        metrics = {}
        
        n_clusters = len(set(labels))
        n_samples = len(labels)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if n_clusters > 1 and n_clusters < n_samples:
            try:
                metrics['Silhouette Score'] = silhouette_score(vectors, labels)
            except:
                metrics['Silhouette Score'] = np.nan
            
            try:
                metrics['Calinski-Harabasz'] = calinski_harabasz_score(vectors, labels)
            except:
                metrics['Calinski-Harabasz'] = np.nan
            
            try:
                metrics['Davies-Bouldin'] = davies_bouldin_score(vectors, labels)
            except:
                metrics['Davies-Bouldin'] = np.nan
        
        # –ú–µ—Ç—Ä–∏–∫–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        if true_labels is not None and len(true_labels) == len(labels):
            try:
                metrics['Adjusted Rand Index'] = adjusted_rand_score(true_labels, labels)
            except:
                metrics['Adjusted Rand Index'] = np.nan
            
            try:
                metrics['Adjusted Mutual Info'] = adjusted_mutual_info_score(true_labels, labels)
            except:
                metrics['Adjusted Mutual Info'] = np.nan
            
            try:
                metrics['Homogeneity'] = homogeneity_score(true_labels, labels)
            except:
                metrics['Homogeneity'] = np.nan
            
            try:
                metrics['Completeness'] = completeness_score(true_labels, labels)
            except:
                metrics['Completeness'] = np.nan
            
            try:
                metrics['V-measure'] = v_measure_score(true_labels, labels)
            except:
                metrics['V-measure'] = np.nan
            
            try:
                metrics['Fowlkes-Mallows'] = fowlkes_mallows_score(true_labels, labels)
            except:
                metrics['Fowlkes-Mallows'] = np.nan
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics['Number of Clusters'] = n_clusters
        metrics['Number of Samples'] = n_samples
        metrics['Noise Points'] = sum(labels == -1) if -1 in labels else 0
        
        # –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_sizes = [sum(labels == i) for i in range(n_clusters)]
        if cluster_sizes:
            metrics['Largest Cluster'] = max(cluster_sizes)
            metrics['Smallest Cluster'] = min(cluster_sizes)
            metrics['Avg Cluster Size'] = np.mean(cluster_sizes)
            metrics['Cluster Size Std'] = np.std(cluster_sizes)
        
        return metrics

class AdvancedVisualization:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    @staticmethod
    def plot_clusters_plotly(vectors, labels, method='PCA', title=None):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Plotly"""
        
        if method == 'PCA':
            reducer = PCA(n_components=2)
            embeddings_2d = reducer.fit_transform(vectors)
            title = title or '–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (PCA)'
            explained_var = reducer.explained_variance_ratio_.sum()
            title += f' (–û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è: {explained_var:.2%})'
            
        elif method == 'TSNE':
            reducer = TSNE(n_components=2, random_state=42, 
                          perplexity=min(30, len(vectors)-1))
            embeddings_2d = reducer.fit_transform(vectors)
            title = title or '–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (t-SNE)'
            
        elif method == 'UMAP':
            reducer = umap.UMAP(n_components=2, random_state=42)
            embeddings_2d = reducer.fit_transform(vectors)
            title = title or '–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (UMAP)'
            
        else:  # Truncated SVD
            reducer = TruncatedSVD(n_components=2, random_state=42)
            embeddings_2d = reducer.fit_transform(vectors)
            title = title or '–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (Truncated SVD)'
        
        df_plot = pd.DataFrame({
            'x': embeddings_2d[:, 0],
            'y': embeddings_2d[:, 1],
            'cluster': labels,
            'cluster_label': [f'–ö–ª–∞—Å—Ç–µ—Ä {l}' if l != -1 else '–®—É–º' for l in labels]
        })
        
        fig = px.scatter(
            df_plot, x='x', y='y', color='cluster_label',
            title=title,
            hover_data={'cluster': True},
            color_discrete_sequence=px.colors.qualitative.Set3
        )
        
        fig.update_layout(
            width=800,
            height=600,
            showlegend=True
        )
        
        return fig
    
    @staticmethod
    def plot_metrics_comparison(metrics_df):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""
        metrics_to_plot = ['Silhouette Score', 'Calinski-Harabasz', 'Davies-Bouldin']
        available_metrics = [m for m in metrics_to_plot if m in metrics_df.columns]
        
        if not available_metrics:
            return None
        
        fig = go.Figure()
        
        for metric in available_metrics:
            fig.add_trace(go.Bar(
                name=metric,
                x=metrics_df.index,
                y=metrics_df[metric],
                text=metrics_df[metric].round(3),
                textposition='auto',
            ))
        
        fig.update_layout(
            title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
            xaxis_title="–ú–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
            yaxis_title="–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏",
            barmode='group',
            height=500
        )
        
        return fig

def find_optimal_eps(vectors, min_samples=5):
    """–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ EPS –¥–ª—è DBSCAN"""
    neighbors = NearestNeighbors(n_neighbors=min_samples)
    neighbors_fit = neighbors.fit(vectors)
    distances, indices = neighbors_fit.kneighbors(vectors)
    distances = np.sort(distances[:, -1], axis=0)
    return distances

def main():
    st.set_page_config(
        page_title="–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤",
        page_icon="üîç",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üîç –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    st.markdown("---")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–æ–≤
    processor = AdvancedTextProcessor()
    vectorizer = AdvancedVectorizer()
    clustering = AdvancedClusteringMethods()
    metrics_calc = MetricsCalculator()
    viz = AdvancedVisualization()
    
    # –°–∞–π–¥–±–∞—Ä —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    uploaded_file = st.sidebar.file_uploader(
        "üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ",
        type=['csv', 'txt'],
        help="CSV —Å –∫–æ–ª–æ–Ω–∫–æ–π —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ TXT —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ"
    )
    
    # –î–µ–º–æ –¥–∞–Ω–Ω—ã–µ
    use_demo = st.sidebar.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ–º–æ –¥–∞–Ω–Ω—ã–µ", value=True)
    
    if uploaded_file is None and not use_demo:
        st.info("üëÜ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–µ–º–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
                st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω CSV —Ñ–∞–π–ª —Å {len(df)} —Å—Ç—Ä–æ–∫–∞–º–∏")
                
                # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–∫–∏
                text_cols = [col for col in df.columns if df[col].dtype == 'object']
                if text_cols:
                    text_column = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É", text_cols)
                    texts = df[text_column].dropna().astype(str).tolist()
                    
                    # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–∫–∏ —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                    all_cols = df.columns.tolist()
                    if len(all_cols) > 1:
                        label_col = st.sidebar.selectbox(
                            "–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", 
                            ['–ù–µ—Ç'] + [col for col in all_cols if col != text_column]
                        )
                        if label_col != '–ù–µ—Ç':
                            true_labels = df[label_column].dropna().astype(str).tolist()
                        else:
                            true_labels = None
                    else:
                        true_labels = None
                else:
                    st.error("‚ùå –í —Ñ–∞–π–ª–µ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫")
                    return
            else:
                text_data = uploaded_file.read().decode('utf-8')
                texts = [line.strip() for line in text_data.split('\n') if line.strip()]
                st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫")
                true_labels = None
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
            return
    else:
        # –î–µ–º–æ –¥–∞–Ω–Ω—ã–µ —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        demo_data = [
            ("–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å–µ–≥–æ–¥–Ω—è –æ—á–µ–Ω—å –ø–æ–ø—É–ª—è—Ä–Ω—ã", "AI"),
            ("–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á", "AI"),
            ("–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –ø–æ–Ω–∏–º–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é —Ä–µ—á—å", "NLP"),
            ("–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é", "Data Science"),
            ("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ —Ç—Ä–µ–Ω–¥—ã", "Data Science"),
            ("–ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Ö—Ä–∞–Ω—è—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞", "Databases"),
            ("SQL —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–º–∏ –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö", "Databases"),
            ("–í–µ–± —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–∑–¥–∞–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –∏ —Å–µ—Ä–≤–µ—Ä–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è", "Web"),
            ("–§—Ä–æ–Ω—Ç–µ–Ω–¥ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å", "Web"),
            ("–ë—ç–∫–µ–Ω–¥ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Å–µ—Ä–≤–µ—Ä–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –∏ –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö", "Web"),
            ("–ú–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –¥–ª—è iOS –∏ Android –ø–ª–∞—Ç—Ñ–æ—Ä–º", "Mobile"),
            ("–ö—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –û–°", "Mobile"),
            ("–û–±–ª–∞—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Ä–µ—Å—É—Ä—Å—ã —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "Cloud"),
            ("Amazon AWS –∏ Microsoft Azure –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –æ–±–ª–∞—á–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã", "Cloud"),
            ("–ö–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∑–∞—â–∏—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç –Ω–µ—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞", "Security"),
            ("–®–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "Security")
        ]
        texts = [item[0] for item in demo_data]
        true_labels = [item[1] for item in demo_data]
        st.info("üîÆ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    st.sidebar.subheader("üîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞")
    use_stemming = st.sidebar.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–µ–º–º–∏–Ω–≥", value=True)
    
    st.sidebar.subheader("üìä –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è")
    vectorization_method = st.sidebar.selectbox(
        "–ú–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏",
        ['TF-IDF', 'BM25', 'Count Vectorizer', 'Sentence Transformers', 'Word2Vec', 'FastText', 'Doc2Vec']
    )
    
    st.sidebar.subheader("üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
    clustering_methods = st.sidebar.multiselect(
        "–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
        ['KMeans', 'DBSCAN', 'Hierarchical', 'GaussianMixture', 'Spectral', 'HDBSCAN', 'BIRCH'],
        default=['KMeans', 'DBSCAN', 'Hierarchical']
    )
    
    n_clusters = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 2, 15, 5)
    
    st.sidebar.subheader("üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è")
    viz_method = st.sidebar.selectbox(
        "–ú–µ—Ç–æ–¥ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏",
        ['PCA', 'TSNE', 'UMAP', 'Truncated SVD']
    )
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    if st.sidebar.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑", type="primary"):
        
        with st.spinner("üîÑ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤..."):
            try:
                # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
                tokenized_texts = processor.preprocess_texts(texts, use_stemming=use_stemming)
                
                st.write(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(tokenized_texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                
                # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
                if vectorization_method == 'TF-IDF':
                    vectors = vectorizer.fit_tfidf(tokenized_texts)
                elif vectorization_method == 'BM25':
                    vectors = vectorizer.fit_bm25(tokenized_texts)
                elif vectorization_method == 'Count Vectorizer':
                    vectors = vectorizer.fit_count(tokenized_texts)
                elif vectorization_method == 'Sentence Transformers':
                    vectors = vectorizer.fit_sentence_transformers(texts)
                elif vectorization_method == 'Word2Vec':
                    vectors = vectorizer.fit_word2vec(tokenized_texts)
                elif vectorization_method == 'FastText':
                    vectors = vectorizer.fit_fasttext(tokenized_texts)
                else:  # Doc2Vec
                    vectors = vectorizer.fit_doc2vec(tokenized_texts)
                
                st.success(f"‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {vectors.shape}")
                
                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                all_results = {}
                all_metrics = {}
                
                for method in clustering_methods:
                    with st.spinner(f"üîÑ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è {method} –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è..."):
                        try:
                            if method == 'KMeans':
                                labels, model = clustering.kmeans(vectors, n_clusters=n_clusters)
                            elif method == 'DBSCAN':
                                labels, model = clustering.dbscan(vectors)
                            elif method == 'Hierarchical':
                                labels, model = clustering.hierarchical(vectors, n_clusters=n_clusters)
                            elif method == 'GaussianMixture':
                                labels, model = clustering.gaussian_mixture(vectors, n_components=n_clusters)
                            elif method == 'Spectral':
                                labels, model = clustering.spectral(vectors, n_clusters=n_clusters)
                            elif method == 'HDBSCAN':
                                labels, model = clustering.hdbscan_method(vectors)
                            else:  # BIRCH
                                labels, model = clustering.birch(vectors, n_clusters=n_clusters)
                            
                            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
                            metrics = metrics_calc.calculate_all_metrics(vectors, labels, true_labels)
                            all_results[method] = labels
                            all_metrics[method] = metrics
                            
                        except Exception as e:
                            st.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ {method}: {e}")
                
                # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫
                st.subheader("üìä –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫")
                
                metrics_df = pd.DataFrame(all_metrics).T
                
                # –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã
                def highlight_extremes(s):
                    if s.dtype in [np.float64, np.int64]:
                        is_max = s == s.max()
                        is_min = s == s.min()
                        return ['background-color: lightgreen' if max_val else 
                                'background-color: lightcoral' if min_val else '' 
                                for max_val, min_val in zip(is_max, is_min)]
                    return [''] * len(s)
                
                styled_metrics = metrics_df.style.format("{:.3f}").apply(highlight_extremes)
                st.dataframe(styled_metrics, use_container_width=True)
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
                comparison_fig = viz.plot_metrics_comparison(metrics_df)
                if comparison_fig:
                    st.plotly_chart(comparison_fig, use_container_width=True)
                
                # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
                for method in clustering_methods:
                    if method in all_results:
                        st.subheader(f"üîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {method}")
                        
                        labels = all_results[method]
                        metrics = all_metrics[method]
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.write("**üìà –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**")
                            for metric_name in ['Silhouette Score', 'Calinski-Harabasz', 'Davies-Bouldin']:
                                if metric_name in metrics:
                                    st.metric(metric_name, f"{metrics[metric_name]:.3f}")
                        
                        with col2:
                            st.write("**üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:**")
                            cluster_counts = pd.Series(labels).value_counts().sort_index()
                            st.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {metrics['Number of Clusters']}")
                            st.write(f"–¢–æ—á–µ–∫ —à—É–º–∞: {metrics['Noise Points']}")
                        
                        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
                        fig = viz.plot_clusters_plotly(vectors, labels, method=viz_method, 
                                                     title=f'{method} –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è')
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                        st.write("**üîç –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:**")
                        
                        results_df = pd.DataFrame({
                            '–¢–µ–∫—Å—Ç': texts[:len(labels)],
                            '–¢–æ–∫–µ–Ω—ã': [' '.join(tokens) for tokens in tokenized_texts[:len(labels)]],
                            '–ö–ª–∞—Å—Ç–µ—Ä': labels
                        })
                        
                        if true_labels is not None:
                            results_df['–ò—Å—Ç–∏–Ω–Ω–∞—è_–º–µ—Ç–∫–∞'] = true_labels[:len(labels)]
                        
                        for cluster_id in sorted(set(labels)):
                            with st.expander(f"üìÇ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({sum(labels == cluster_id)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)"):
                                cluster_data = results_df[results_df['–ö–ª–∞—Å—Ç–µ—Ä'] == cluster_id]
                                
                                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞
                                st.write("**–ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:**")
                                for i, (idx, row) in enumerate(cluster_data.head(3).iterrows()):
                                    st.write(f"{i+1}. {row['–¢–µ–∫—Å—Ç']}")
                                
                                # –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
                                all_tokens = []
                                for tokens_str in cluster_data['–¢–æ–∫–µ–Ω—ã']:
                                    all_tokens.extend(tokens_str.split())
                                
                                if all_tokens:
                                    common_words = Counter(all_tokens).most_common(8)
                                    st.write("**–¢–æ–ø-8 —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤:**")
                                    words_df = pd.DataFrame(common_words, columns=['–°–ª–æ–≤–æ', '–ß–∞—Å—Ç–æ—Ç–∞'])
                                    st.dataframe(words_df, use_container_width=True)
                                
                                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
                                if true_labels is not None and '–ò—Å—Ç–∏–Ω–Ω–∞—è_–º–µ—Ç–∫–∞' in cluster_data.columns:
                                    true_labels_dist = cluster_data['–ò—Å—Ç–∏–Ω–Ω–∞—è_–º–µ—Ç–∫–∞'].value_counts()
                                    st.write("**–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫:**")
                                    st.dataframe(true_labels_dist, use_container_width=True)
                        
                        st.markdown("---")
                
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞: {e}")
                st.code(traceback.format_exc())

if __name__ == "__main__":
    main()